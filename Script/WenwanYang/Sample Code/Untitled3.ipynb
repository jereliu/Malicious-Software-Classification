{
 "metadata": {
  "name": "",
  "signature": "sha256:93d4a044705b3d89499cffc7e9f601aeb19241588de18a4f4fbab0a1c3d9dd4d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "from collections import Counter\n",
      "try:\n",
      "    import xml.etree.cElementTree as ET\n",
      "except ImportError:\n",
      "    import xml.etree.ElementTree as ET\n",
      "\n",
      "#bread and butter\n",
      "import nltk\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from scipy import sparse\n",
      "\n",
      "#toyz\n",
      "from sklearn import cross_validation as cv\n",
      "from sklearn import ensemble as es\n",
      "os.chdir(\"/Users/Vivian/Desktop/data\")\n",
      "import util\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "################################################\n",
      "#### 1. Auxilliary Funcs #######################\n",
      "################################################\n",
      "def print_full(x):\n",
      "    #print all rows of a panda dataframe\n",
      "    pd.set_option('display.max_rows', len(x))\n",
      "    print(x)\n",
      "    pd.reset_option('display.max_rows')\n",
      "\n",
      "\n",
      "def Accuracy(clf_preds, t_train):\n",
      "    clf_missId = ((clf_preds - t_train) != 0)\n",
      "    clf_miss = t_train[(clf_preds - t_train) != 0]\n",
      "    \n",
      "    rate = 1 - np.mean(clf_missId) #error rate\n",
      "    return rate, clf_miss\n",
      "\n",
      "def name_for_feature(command, t_train, X_train, feature_dict):\n",
      "    try:\n",
      "        col_id = feature_dict[command]\n",
      "    except KeyError:\n",
      "        print (\"command not exist\")\n",
      "        raise KeyError\n",
      "        \n",
      "    t_train = t_train.reshape(1, np.max(t_train.shape))\n",
      "    ftData = X_train.T[col_id]\n",
      "\n",
      "    nmIdx = t_train[np.array(ftData>0)]\n",
      "    out = [util.malware_classes[i] for i in nmIdx]\n",
      "    out = stats.itemfreq(out)\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def pruneFeatures(minFreq, X_train_dense, global_feat_dict):\n",
      "    print \"pruning training features...\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    #featureFreq = [sum(feature != 0) for feature in X_train_dense.T]\n",
      "    #featureFreq_pd = pd.Series(featureFreq, global_feat_dict.keys())\n",
      "    featureFreq = []\n",
      "    X_train_denseT = X_train_dense.T\n",
      "    i = 0\n",
      "    i_max = len(X_train_denseT)\n",
      "\n",
      "    for feature in X_train_denseT:\n",
      "        featureFreq.append(sum(feature != 0))\n",
      "        i += 1\n",
      "        if i % 5000 == 0: \n",
      "            print \"Obtaining featureFreq: \" + \\\n",
      "                    str(round(float(i)*100/i_max, 3)) + \"%\"\n",
      "\n",
      "    print \"obtaining prunId...\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    prunId = []\n",
      "    i = 0\n",
      "    for item in featureFreq: \n",
      "        if item > minFreq: prunId.append(i)\n",
      "        i += 1\n",
      "        if i % 5000 == 0: \n",
      "            print \"Obtaining prunId: \" + \\\n",
      "                    str(round(float(i)*100/i_max, 3)) + \"%\"\n",
      "            sys.stdout.flush()\n",
      "        \n",
      "    print \"obtaining prunId...Done!\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    print \"Pruning Design Matrix... \"; sys.stdout.flush()\n",
      "    X_train_prune = X_train_dense.T[prunId].T #update X\n",
      "    print \"Pruning Design Matrix...Done!\"\n",
      "\n",
      "    global_feat_dict_prune = Counter() #update global_feat_dict\n",
      "    \n",
      "    dictList = global_feat_dict.items()\n",
      "    dictList_prune = np.array(dictList)[np.array(prunId)]\n",
      "    \n",
      "    for i in range(len(dictList_prune)):\n",
      "        global_feat_dict_prune[dictList_prune[i][0]] = dictList_prune[i][1]\n",
      "        if i % 5000 == 0: \n",
      "            print \"Pruning feature dict: \" + \\\n",
      "                str(round(float(i)*100/len(dictList_prune), 3)) + \"%\"\n",
      "            sys.stdout.flush()\n",
      "\n",
      "    print \"done pruning training features!\"\n",
      "    return X_train_prune, global_feat_dict_prune, featureFreq, prunId\n",
      "\n",
      "    \n",
      "################################################\n",
      "\n",
      "################################\n",
      "#### 2. IO functions\n",
      "################################\n",
      "\n",
      "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      ffs are a list of feature-functions.\n",
      "      direc is a directory containing xml files (expected to be train or test).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "\n",
      "    returns: \n",
      "      a sparse design matrix, a dict mapping features to column-numbers,\n",
      "      a vector of target classes, and a list of system-call-history ids in order \n",
      "      of their rows in the design matrix.\n",
      "      \n",
      "      Note: the vector of target classes returned will contain the true indices of the\n",
      "      target classes on the training data, but will contain only -1's on the test\n",
      "      data\n",
      "    \"\"\"\n",
      "    \n",
      "    fds = [] # list of feature dicts\n",
      "    classes = []\n",
      "    ids = [] \n",
      "    \n",
      "    i = 0\n",
      "    N = str(len(os.listdir(direc)))\n",
      "    \n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        ids.append(id_str)\n",
      "        # add target class if this is training data\n",
      "        try:\n",
      "            classes.append(util.malware_classes.index(clazz))\n",
      "        except ValueError:\n",
      "            # we should only fail to find the label in our list of malware classes\n",
      "            # if this is test data, which always has an \"X\" label\n",
      "            assert clazz == \"X\"\n",
      "            classes.append(-1)\n",
      "        rowfd = {}\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        [rowfd.update(ff(tree)) for ff in ffs]\n",
      "        fds.append(rowfd)\n",
      "        \n",
      "        i += 1\n",
      "        if i%100 == 0: \n",
      "            print \"Progress: \" + str(i) + \"/\" + N\n",
      "            sys.stdout.flush()\n",
      "        \n",
      "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
      "    return X, feat_dict, np.array(classes), ids\n",
      "\n",
      "\n",
      "def make_design_mat(fds, global_feat_dict = None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      fds is a list of feature dicts (one for each row).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "       \n",
      "    returns: \n",
      "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
      "        the union of features defined in any of the fds \n",
      "    \"\"\"\n",
      "    if global_feat_dict is None:\n",
      "        all_feats = set()\n",
      "        [all_feats.update(fd.keys()) for fd in fds]\n",
      "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
      "    else:\n",
      "        feat_dict = global_feat_dict\n",
      "        \n",
      "    cols = []\n",
      "    rows = []\n",
      "    data = []        \n",
      "    for i in xrange(len(fds)):\n",
      "        temp_cols = []\n",
      "        temp_data = []\n",
      "        for feat,val in fds[i].iteritems():\n",
      "            try:\n",
      "                # update temp_cols iff update temp_data\n",
      "                temp_cols.append(feat_dict[feat])\n",
      "                temp_data.append(val)\n",
      "            except KeyError as ex:\n",
      "                if global_feat_dict is not None:\n",
      "                    pass  # new feature in test data; nbd\n",
      "                else:\n",
      "                    raise ex\n",
      "\n",
      "        # all fd's features in the same row\n",
      "        k = len(temp_cols)\n",
      "        cols.extend(temp_cols)\n",
      "        data.extend(temp_data)\n",
      "        rows.extend([i]*k)\n",
      "\n",
      "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
      "   \n",
      "\n",
      "    X = sparse.csr_matrix(\n",
      "    (np.array(data),(np.array(rows), np.array(cols))), \n",
      "    shape=(len(fds), len(feat_dict))\n",
      "    )\n",
      "    \n",
      "    return X, feat_dict\n",
      "    \n",
      "################################################\n",
      "\n",
      "################################\n",
      "#### 3. Feature Vectors\n",
      "################################\n",
      "\n",
      "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
      "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
      "# feature-names to numeric values.\n",
      "## TODO: \n",
      "##    1. DLL Type and Address.\n",
      "##    2. enum_values, explorer\n",
      "##    3. create_namedpipe\n",
      "##    4. create_thread_remote\n",
      "\n",
      "\n",
      "def get_all_keys(tree):\n",
      "    key_list = Counter()\n",
      "    tag_exclusion_set = [\"load_dll\"]\n",
      "    \n",
      "    in_all_section = False\n",
      "\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            #if el.tag in tag_set\n",
      "            for key in el.keys():\n",
      "                if key not in tag_exclusion_set:\n",
      "                    key_list_name = \\\n",
      "                        (el.tag + \"_\" + key + \"-\" + el.get(key)).lower()\n",
      "                    key_list[key_list_name] += 1\n",
      "    return key_list\n",
      "\n",
      "\n",
      "def dll_type(tree, name_only = False):\n",
      "    #type and address of loaded DLLs\n",
      "    dll_list = {\"name\": [], \"extn\": [], \"addr\": []} \n",
      "\n",
      "    for el in tree.iter():\n",
      "        # obtain DLL target in element\n",
      "        if el.tag == \"load_dll\":\n",
      "            dll_name = el.get('filename')\n",
      "            #split filename and file_address\n",
      "            try:\n",
      "                key_bag = dll_name.split(\"\\\\\")\n",
      "                dll_name = \"dll_name-\" + key_bag[len(key_bag) - 1]\n",
      "                dll_addr = \"dll_addr-\" + \"//\".join(key_bag[:(len(key_bag) - 1)])\n",
      "            except AttributeError:\n",
      "                dll_name = \"dll_name-DLL_FILE_NOT_SPECIFIED\"\n",
      "                dll_addr = \"dll_addr-\" + \"//\".join(key_bag)\n",
      "            \n",
      "            if len(dll_name.split(\".\")) == 2:\n",
      "                key_bag = dll_name.split(\".\")\n",
      "                dll_name = key_bag[0]\n",
      "                dll_extn = \"dll_extn-\" + key_bag[1]\n",
      "            else:\n",
      "                dll_extn = \"dll_extn-\"\n",
      "            \n",
      "            dll_name = dll_name.lower()\n",
      "            dll_extn = dll_extn.lower()\n",
      "            dll_addr = dll_addr.lower()\n",
      "            #TODO: convert to lower case\n",
      "            dll_list[\"name\"].append(dll_name)\n",
      "            dll_list[\"extn\"].append(dll_extn)\n",
      "            dll_list[\"addr\"].append(dll_addr)\n",
      "\n",
      "    dll_list[\"name\"] = stats.itemfreq(dll_list[\"name\"])\n",
      "    dll_list[\"extn\"] = stats.itemfreq(dll_list[\"extn\"])    \n",
      "    dll_list[\"addr\"] = stats.itemfreq(dll_list[\"addr\"])    \n",
      "    dll_list_join = concatenate([dll_list[\"name\"], \\\n",
      "                    dll_list[\"extn\"], dll_list[\"addr\"]])\n",
      "\n",
      "    dll_name_counter = Counter()\n",
      "    for item in dll_list_join: \n",
      "        dll_name_counter[item[0]] = int(item[1])\n",
      "\n",
      "    return dll_name_counter\n",
      "\n",
      "\n",
      "def call_freq(tree, name_only = False):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    callz = []\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            callz.append(el.tag)\n",
      "\n",
      "    # finally, count the frequencies\n",
      "    freqList = stats.itemfreq(callz)   \n",
      "    \n",
      "    if name_only == True:        \n",
      "        c = set(callz)\n",
      "    else: \n",
      "        c = Counter()\n",
      "        for item in freqList: c[\"sys_call-\" +item[0]] = int(item[1])\n",
      "\n",
      "    return c\n",
      "\n",
      "\n",
      "def call_eigen(tree, call_dict, name_only = False):\n",
      "    #TODO: Implement this!\n",
      "    c = []\n",
      "    return c\n",
      "\n",
      "\n",
      "def first_last_system_call_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            if first:\n",
      "                c[\"first_call-\"+el.tag] = 1\n",
      "                first = False\n",
      "            last_call = el.tag  # update last call seen\n",
      "            \n",
      "    # finally, mark last call seen\n",
      "    c[\"last_call-\"+last_call] = 1\n",
      "    return c\n",
      "\n",
      "def system_call_count_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
      "      made by an executable (summed over all processes)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c['num_system_calls'] += 1\n",
      "    return c\n",
      "\n",
      "\n",
      "################################\n",
      "#### 4. Variables of Interest\n",
      "################################\n",
      "\n",
      "\n",
      "def call_type(direc):\n",
      "    names = set() # list of feature dicts\n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        newnames = call_freq(tree, name_only = True)\n",
      "        names = names.union(newnames)\n",
      "    out_names = names\n",
      "    return out_names\n",
      "\n",
      "def call_freq_emp(direc):\n",
      "    names = dict() # list of feature dicts\n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        newnames = call_freq(tree)\n",
      "        for k in newnames.iterkeys():\n",
      "            if k in names: names[k] = names[k] + newnames[k]\n",
      "            else: names[k] = newnames[k]\n",
      "            \n",
      "    out_names = names\n",
      "    return out_names\n",
      "    \n",
      "def call_freq_byType(direc):\n",
      "    #initiate class dependent dictionary\n",
      "    name_byType = dict() # list of feature dicts\n",
      "    for TypeName in util.malware_classes:\n",
      "        name_byType[TypeName] = dict()\n",
      " \n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features: first create new feature\n",
      "        newnames = call_freq(tree)\n",
      "        # \n",
      "        curNames = name_byType[clazz]\n",
      "        for k in newnames.iterkeys():\n",
      "            if k in curNames: \n",
      "                curNames[k] = curNames[k] + newnames[k]\n",
      "            else: curNames[k] = newnames[k]\n",
      "        name_byType[clazz] = curNames\n",
      "        \n",
      "    out_names = name_byType\n",
      "    return out_names\n",
      "    \n",
      "def dll_type_2(tree, name_only = False):\n",
      "    dll_list = {\"name\": [], \"addr\": []} \n",
      "\n",
      "    for el in tree.iter():\n",
      "        # obtain DLL target in element\n",
      "        if el.tag == \"load_dll\":\n",
      "            dll_name = el.get('filename')\n",
      "            #split filename and file_address\n",
      "            key_bag = dll_name.split(\"\\\\\")\n",
      "            \n",
      "            dll_name = key_bag[len(key_bag) - 1]\n",
      "            dll_addr = \"//\".join(key_bag[:(len(key_bag) - 1)])\n",
      "            #TODO: convert to lower case\n",
      "\n",
      "            dll_list[\"name\"].append(dll_name)\n",
      "            dll_list[\"addr\"].append(dll_addr)\n",
      "\n",
      "    dll_list[\"name\"] = stats.itemfreq(dll_list[\"name\"])\n",
      "    dll_list[\"addr\"] = stats.itemfreq(dll_list[\"addr\"])    \n",
      "    dll_list_join = concatenate([dll_list[\"name\"], dll_list[\"addr\"]])\n",
      "\n",
      "    dll_name_counter = Counter()\n",
      "    for item in dll_list_join: \n",
      "        dll_name_counter[item[0]] = int(item[1])\n",
      "\n",
      "    return dll_name_counter\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_dir = \"train\"\n",
      "ffs = [first_last_system_call_feats, system_call_count_feats, call_freq]\n",
      "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 3000/3086\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_dir = \"test\"\n",
      "\n",
      "ffs = [first_last_system_call_feats, system_call_count_feats, call_freq]\n",
      "\n",
      "X_test, global_feat_dict, t_test, test_ids = extract_feats(ffs, test_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 3000/3086\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "x = range(60, 100, 2)\n",
      "y = []\n",
      "result=[]\n",
      "for i in range(30,300,2):\n",
      "    print i\n",
      "    for j in range(16,32,2):\n",
      "\n",
      "    \n",
      "        clf4 = RandomForestClassifier(n_estimators=20, criterion=\"gini\",max_features=i,max_depth=j)\n",
      "        y_pred4 = clf4.fit(X_train.toarray(), t_train).predict(X_test.todense())\n",
      "\n",
      "        y.append([i,j,(t_train != y_pred4).sum()])\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30\n",
        "32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "44"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "46"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "48"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "62"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "66"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "68"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "70"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "74"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "76"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "78"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "82"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "86"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "90"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "92"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "94"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "96"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "102"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "104"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "106"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "108"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "110"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "112"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "114"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "116"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "118"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "120"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "124"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "126"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "128"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "130"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "132"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "134"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "max_features must be in (0, n_features]",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-8e24fc82ca7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0my_pred4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_train\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 279\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \"\"\"\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(trees, forest, X, y, sample_weight, verbose)\u001b[0m\n\u001b[1;32m     87\u001b[0m             tree.fit(X, y,\n\u001b[1;32m     88\u001b[0m                      \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                      check_input=False)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_mask, X_argsorted, check_input, sample_weight)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_depth must be greater than zero. \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_features must be in (0, n_features]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n",
        "\u001b[0;31mValueError\u001b[0m: max_features must be in (0, n_features]"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[[30, 16, 46],\n",
        " [30, 18, 38],\n",
        " [30, 20, 36],\n",
        " [30, 22, 33],\n",
        " [30, 24, 35],\n",
        " [30, 26, 33],\n",
        " [30, 28, 33],\n",
        " [30, 30, 35],\n",
        " [32, 16, 55],\n",
        " [32, 18, 37],\n",
        " [32, 20, 34],\n",
        " [32, 22, 33],\n",
        " [32, 24, 33],\n",
        " [32, 26, 33],\n",
        " [32, 28, 34],\n",
        " [32, 30, 31],\n",
        " [34, 16, 45],\n",
        " [34, 18, 42],\n",
        " [34, 20, 37],\n",
        " [34, 22, 36],\n",
        " [34, 24, 34],\n",
        " [34, 26, 32],\n",
        " [34, 28, 31],\n",
        " [34, 30, 33],\n",
        " [36, 16, 49],\n",
        " [36, 18, 41],\n",
        " [36, 20, 34],\n",
        " [36, 22, 38],\n",
        " [36, 24, 33],\n",
        " [36, 26, 32],\n",
        " [36, 28, 33],\n",
        " [36, 30, 30],\n",
        " [38, 16, 42],\n",
        " [38, 18, 40],\n",
        " [38, 20, 37],\n",
        " [38, 22, 33],\n",
        " [38, 24, 33],\n",
        " [38, 26, 35],\n",
        " [38, 28, 35],\n",
        " [38, 30, 33],\n",
        " [40, 16, 52],\n",
        " [40, 18, 42],\n",
        " [40, 20, 39],\n",
        " [40, 22, 33],\n",
        " [40, 24, 35],\n",
        " [40, 26, 32],\n",
        " [40, 28, 32],\n",
        " [40, 30, 31],\n",
        " [42, 16, 41],\n",
        " [42, 18, 46],\n",
        " [42, 20, 36],\n",
        " [42, 22, 35],\n",
        " [42, 24, 34],\n",
        " [42, 26, 32],\n",
        " [42, 28, 33],\n",
        " [42, 30, 32],\n",
        " [44, 16, 43],\n",
        " [44, 18, 38],\n",
        " [44, 20, 35],\n",
        " [44, 22, 35],\n",
        " [44, 24, 35],\n",
        " [44, 26, 34],\n",
        " [44, 28, 33],\n",
        " [44, 30, 33],\n",
        " [46, 16, 50],\n",
        " [46, 18, 38],\n",
        " [46, 20, 33],\n",
        " [46, 22, 33],\n",
        " [46, 24, 31],\n",
        " [46, 26, 34],\n",
        " [46, 28, 34],\n",
        " [46, 30, 32],\n",
        " [48, 16, 49],\n",
        " [48, 18, 37],\n",
        " [48, 20, 35],\n",
        " [48, 22, 36],\n",
        " [48, 24, 32],\n",
        " [48, 26, 33],\n",
        " [48, 28, 37],\n",
        " [48, 30, 32],\n",
        " [50, 16, 50],\n",
        " [50, 18, 37],\n",
        " [50, 20, 37],\n",
        " [50, 22, 33],\n",
        " [50, 24, 33],\n",
        " [50, 26, 31],\n",
        " [50, 28, 33],\n",
        " [50, 30, 34],\n",
        " [52, 16, 49],\n",
        " [52, 18, 40],\n",
        " [52, 20, 37],\n",
        " [52, 22, 37],\n",
        " [52, 24, 39],\n",
        " [52, 26, 32],\n",
        " [52, 28, 33],\n",
        " [52, 30, 33],\n",
        " [54, 16, 50],\n",
        " [54, 18, 44],\n",
        " [54, 20, 31],\n",
        " [54, 22, 33],\n",
        " [54, 24, 31],\n",
        " [54, 26, 35],\n",
        " [54, 28, 33],\n",
        " [54, 30, 31],\n",
        " [56, 16, 44],\n",
        " [56, 18, 40],\n",
        " [56, 20, 37],\n",
        " [56, 22, 37],\n",
        " [56, 24, 31],\n",
        " [56, 26, 33],\n",
        " [56, 28, 32],\n",
        " [56, 30, 33],\n",
        " [58, 16, 53],\n",
        " [58, 18, 45],\n",
        " [58, 20, 35],\n",
        " [58, 22, 33],\n",
        " [58, 24, 32],\n",
        " [58, 26, 33],\n",
        " [58, 28, 34],\n",
        " [58, 30, 35],\n",
        " [60, 16, 45],\n",
        " [60, 18, 42],\n",
        " [60, 20, 34],\n",
        " [60, 22, 35],\n",
        " [60, 24, 32],\n",
        " [60, 26, 36],\n",
        " [60, 28, 34],\n",
        " [60, 30, 35],\n",
        " [62, 16, 49],\n",
        " [62, 18, 39],\n",
        " [62, 20, 40],\n",
        " [62, 22, 31],\n",
        " [62, 24, 34],\n",
        " [62, 26, 32],\n",
        " [62, 28, 33],\n",
        " [62, 30, 31],\n",
        " [64, 16, 46],\n",
        " [64, 18, 40],\n",
        " [64, 20, 37],\n",
        " [64, 22, 41],\n",
        " [64, 24, 34],\n",
        " [64, 26, 37],\n",
        " [64, 28, 36],\n",
        " [64, 30, 34],\n",
        " [66, 16, 48],\n",
        " [66, 18, 37],\n",
        " [66, 20, 38],\n",
        " [66, 22, 32],\n",
        " [66, 24, 34],\n",
        " [66, 26, 33],\n",
        " [66, 28, 35],\n",
        " [66, 30, 36],\n",
        " [68, 16, 44],\n",
        " [68, 18, 41],\n",
        " [68, 20, 34],\n",
        " [68, 22, 35],\n",
        " [68, 24, 32],\n",
        " [68, 26, 33],\n",
        " [68, 28, 35],\n",
        " [68, 30, 35],\n",
        " [70, 16, 48],\n",
        " [70, 18, 41],\n",
        " [70, 20, 38],\n",
        " [70, 22, 35],\n",
        " [70, 24, 35],\n",
        " [70, 26, 31],\n",
        " [70, 28, 33],\n",
        " [70, 30, 35],\n",
        " [72, 16, 53],\n",
        " [72, 18, 41],\n",
        " [72, 20, 32],\n",
        " [72, 22, 38],\n",
        " [72, 24, 34],\n",
        " [72, 26, 33],\n",
        " [72, 28, 32],\n",
        " [72, 30, 35],\n",
        " [74, 16, 44],\n",
        " [74, 18, 37],\n",
        " [74, 20, 35],\n",
        " [74, 22, 35],\n",
        " [74, 24, 37],\n",
        " [74, 26, 31],\n",
        " [74, 28, 31],\n",
        " [74, 30, 36],\n",
        " [76, 16, 50],\n",
        " [76, 18, 41],\n",
        " [76, 20, 32],\n",
        " [76, 22, 38],\n",
        " [76, 24, 32],\n",
        " [76, 26, 32],\n",
        " [76, 28, 32],\n",
        " [76, 30, 33],\n",
        " [78, 16, 47],\n",
        " [78, 18, 47],\n",
        " [78, 20, 44],\n",
        " [78, 22, 36],\n",
        " [78, 24, 33],\n",
        " [78, 26, 37],\n",
        " [78, 28, 35],\n",
        " [78, 30, 34],\n",
        " [80, 16, 50],\n",
        " [80, 18, 36],\n",
        " [80, 20, 35],\n",
        " [80, 22, 35],\n",
        " [80, 24, 31],\n",
        " [80, 26, 34],\n",
        " [80, 28, 32],\n",
        " [80, 30, 33],\n",
        " [82, 16, 52],\n",
        " [82, 18, 46],\n",
        " [82, 20, 36],\n",
        " [82, 22, 35],\n",
        " [82, 24, 35],\n",
        " [82, 26, 33],\n",
        " [82, 28, 33],\n",
        " [82, 30, 36],\n",
        " [84, 16, 47],\n",
        " [84, 18, 42],\n",
        " [84, 20, 36],\n",
        " [84, 22, 35],\n",
        " [84, 24, 31],\n",
        " [84, 26, 32],\n",
        " [84, 28, 32],\n",
        " [84, 30, 34],\n",
        " [86, 16, 52],\n",
        " [86, 18, 43],\n",
        " [86, 20, 41],\n",
        " [86, 22, 35],\n",
        " [86, 24, 35],\n",
        " [86, 26, 35],\n",
        " [86, 28, 33],\n",
        " [86, 30, 31],\n",
        " [88, 16, 47],\n",
        " [88, 18, 43],\n",
        " [88, 20, 39],\n",
        " [88, 22, 37],\n",
        " [88, 24, 33],\n",
        " [88, 26, 32],\n",
        " [88, 28, 33],\n",
        " [88, 30, 31],\n",
        " [90, 16, 45],\n",
        " [90, 18, 35],\n",
        " [90, 20, 38],\n",
        " [90, 22, 36],\n",
        " [90, 24, 34],\n",
        " [90, 26, 33],\n",
        " [90, 28, 38],\n",
        " [90, 30, 38],\n",
        " [92, 16, 46],\n",
        " [92, 18, 41],\n",
        " [92, 20, 36],\n",
        " [92, 22, 34],\n",
        " [92, 24, 33],\n",
        " [92, 26, 35],\n",
        " [92, 28, 33],\n",
        " [92, 30, 32],\n",
        " [94, 16, 46],\n",
        " [94, 18, 44],\n",
        " [94, 20, 33],\n",
        " [94, 22, 37],\n",
        " [94, 24, 37],\n",
        " [94, 26, 36],\n",
        " [94, 28, 37],\n",
        " [94, 30, 32],\n",
        " [96, 16, 44],\n",
        " [96, 18, 36],\n",
        " [96, 20, 39],\n",
        " [96, 22, 34],\n",
        " [96, 24, 32],\n",
        " [96, 26, 33],\n",
        " [96, 28, 32],\n",
        " [96, 30, 34],\n",
        " [98, 16, 41],\n",
        " [98, 18, 45],\n",
        " [98, 20, 36],\n",
        " [98, 22, 37],\n",
        " [98, 24, 35],\n",
        " [98, 26, 34],\n",
        " [98, 28, 32],\n",
        " [98, 30, 34],\n",
        " [100, 16, 49],\n",
        " [100, 18, 44],\n",
        " [100, 20, 41],\n",
        " [100, 22, 35],\n",
        " [100, 24, 33],\n",
        " [100, 26, 34],\n",
        " [100, 28, 39],\n",
        " [100, 30, 37],\n",
        " [102, 16, 45],\n",
        " [102, 18, 43],\n",
        " [102, 20, 42],\n",
        " [102, 22, 36],\n",
        " [102, 24, 37],\n",
        " [102, 26, 35],\n",
        " [102, 28, 35],\n",
        " [102, 30, 37],\n",
        " [104, 16, 50],\n",
        " [104, 18, 45],\n",
        " [104, 20, 39],\n",
        " [104, 22, 39],\n",
        " [104, 24, 34],\n",
        " [104, 26, 34],\n",
        " [104, 28, 33],\n",
        " [104, 30, 33],\n",
        " [106, 16, 54],\n",
        " [106, 18, 40],\n",
        " [106, 20, 33],\n",
        " [106, 22, 33],\n",
        " [106, 24, 34],\n",
        " [106, 26, 32],\n",
        " [106, 28, 34],\n",
        " [106, 30, 31],\n",
        " [108, 16, 46],\n",
        " [108, 18, 45],\n",
        " [108, 20, 42],\n",
        " [108, 22, 34],\n",
        " [108, 24, 37],\n",
        " [108, 26, 30],\n",
        " [108, 28, 33],\n",
        " [108, 30, 34],\n",
        " [110, 16, 45],\n",
        " [110, 18, 41],\n",
        " [110, 20, 35],\n",
        " [110, 22, 33],\n",
        " [110, 24, 34],\n",
        " [110, 26, 33],\n",
        " [110, 28, 34],\n",
        " [110, 30, 34],\n",
        " [112, 16, 53],\n",
        " [112, 18, 42],\n",
        " [112, 20, 33],\n",
        " [112, 22, 36],\n",
        " [112, 24, 33],\n",
        " [112, 26, 35],\n",
        " [112, 28, 33],\n",
        " [112, 30, 32],\n",
        " [114, 16, 53],\n",
        " [114, 18, 35],\n",
        " [114, 20, 41],\n",
        " [114, 22, 37],\n",
        " [114, 24, 34],\n",
        " [114, 26, 33],\n",
        " [114, 28, 36],\n",
        " [114, 30, 33],\n",
        " [116, 16, 48],\n",
        " [116, 18, 43],\n",
        " [116, 20, 38],\n",
        " [116, 22, 37],\n",
        " [116, 24, 36],\n",
        " [116, 26, 35],\n",
        " [116, 28, 36],\n",
        " [116, 30, 34],\n",
        " [118, 16, 53],\n",
        " [118, 18, 38],\n",
        " [118, 20, 39],\n",
        " [118, 22, 35],\n",
        " [118, 24, 37],\n",
        " [118, 26, 36],\n",
        " [118, 28, 35],\n",
        " [118, 30, 32],\n",
        " [120, 16, 46],\n",
        " [120, 18, 43],\n",
        " [120, 20, 36],\n",
        " [120, 22, 38],\n",
        " [120, 24, 35],\n",
        " [120, 26, 33],\n",
        " [120, 28, 33],\n",
        " [120, 30, 36],\n",
        " [122, 16, 56],\n",
        " [122, 18, 42],\n",
        " [122, 20, 39],\n",
        " [122, 22, 36],\n",
        " [122, 24, 34],\n",
        " [122, 26, 36],\n",
        " [122, 28, 35],\n",
        " [122, 30, 32],\n",
        " [124, 16, 46],\n",
        " [124, 18, 38],\n",
        " [124, 20, 37],\n",
        " [124, 22, 37],\n",
        " [124, 24, 35],\n",
        " [124, 26, 31],\n",
        " [124, 28, 33],\n",
        " [124, 30, 33],\n",
        " [126, 16, 50],\n",
        " [126, 18, 42],\n",
        " [126, 20, 38],\n",
        " [126, 22, 43],\n",
        " [126, 24, 35],\n",
        " [126, 26, 35],\n",
        " [126, 28, 32],\n",
        " [126, 30, 34],\n",
        " [128, 16, 41],\n",
        " [128, 18, 40],\n",
        " [128, 20, 36],\n",
        " [128, 22, 39],\n",
        " [128, 24, 36],\n",
        " [128, 26, 33],\n",
        " [128, 28, 31],\n",
        " [128, 30, 32],\n",
        " [130, 16, 51],\n",
        " [130, 18, 40],\n",
        " [130, 20, 38],\n",
        " [130, 22, 34],\n",
        " [130, 24, 34],\n",
        " [130, 26, 35],\n",
        " [130, 28, 35],\n",
        " [130, 30, 34],\n",
        " [132, 16, 54],\n",
        " [132, 18, 42],\n",
        " [132, 20, 43],\n",
        " [132, 22, 34],\n",
        " [132, 24, 37],\n",
        " [132, 26, 32],\n",
        " [132, 28, 32],\n",
        " [132, 30, 33]]"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "\n",
      "with open('parameters.csv', 'w') as csvfile:\n",
      "    fieldnames = ['max_features', 'max_depth',\"Mis_matches\"]\n",
      "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
      "    writer.writeheader()\n",
      "    for i in range(len(y)):\n",
      "        writer.writerow({'max_features': y[i][0], 'max_depth':y[i][1], 'Mis_matches':y[i][2]})\n",
      "      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_dir = \"train\"\n",
      "test_dir = \"test\"\n",
      "ffs = [first_last_system_call_feats, system_call_count_feats, call_freq]   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 3000/3086\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train2 = X_train.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del X_train\n",
      "del t_train\n",
      "del train_ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ParseError",
       "evalue": "not well-formed (invalid token): line 1, column 0",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 1, column 0\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      ffs are a list of feature-functions.\n",
      "      direc is a directory containing xml files (expected to be train or test).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "\n",
      "    returns: \n",
      "      a sparse design matrix, a dict mapping features to column-numbers,\n",
      "      a vector of target classes, and a list of system-call-history ids in order \n",
      "      of their rows in the design matrix.\n",
      "      \n",
      "      Note: the vector of target classes returned will contain the true indices of the\n",
      "      target classes on the training data, but will contain only -1's on the test\n",
      "      data\n",
      "    \"\"\"\n",
      "    \n",
      "    fds = [] # list of feature dicts\n",
      "    classes = []\n",
      "    ids = [] \n",
      "    \n",
      "    i = 0\n",
      "    N = str(len(os.listdir(direc)))\n",
      "    \n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        ids.append(id_str)\n",
      "        \n",
      "        # add target class if this is training data\n",
      "        #try:\n",
      "            #classes.append(util.malware_classes.index(clazz))\n",
      "        #except ValueError:\n",
      "            # we should only fail to find the label in our list of malware classes\n",
      "            # if this is test data, which always has an \"X\" label\n",
      "            #assert clazz == \"X\"\n",
      "            #classes.append(-1)\n",
      "        rowfd = {}\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        [rowfd.update(ff(tree)) for ff in ffs]\n",
      "        fds.append(rowfd)\n",
      "        \n",
      "        i += 1\n",
      "        if i%100 == 0: \n",
      "            print \"Progress: \" + str(i) + \"/\" + N\n",
      "            sys.stdout.flush()\n",
      "        \n",
      "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
      "    return X, feat_dict, np.array(classes), ids\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}