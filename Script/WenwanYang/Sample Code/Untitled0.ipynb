{
 "metadata": {
  "name": "",
  "signature": "sha256:7d417de508eb9d0eaf897cc460f641a240394bc3eac14d1ac4729e6d31309130"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from collections import Counter\n",
      "try:\n",
      "    import xml.etree.cElementTree as ET\n",
      "except ImportError:\n",
      "    import xml.etree.ElementTree as ET\n",
      "import numpy as np\n",
      "from scipy import sparse\n",
      "os.chdir(\"/Users/Vivian/Desktop/data\")\n",
      "import util\n",
      "import pandas as pd\n",
      "import time\n",
      "import sys\n",
      "\n",
      "#bread and butter\n",
      "import nltk\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from scipy import sparse\n",
      "\n",
      "#toyz\n",
      "from sklearn import cross_validation as cv\n",
      "from sklearn import ensemble as es\n",
      "import util\n",
      "import pandas as pd\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###############################################\n",
      "#### 1. Auxilliary Funcs #######################\n",
      "################################################\n",
      "def print_full(x):\n",
      "    #print all rows of a panda dataframe\n",
      "    pd.set_option('display.max_rows', len(x))\n",
      "    print(x)\n",
      "    pd.reset_option('display.max_rows')\n",
      "\n",
      "\n",
      "def Accuracy(clf_preds, t_train):\n",
      "    clf_missId = ((clf_preds - t_train) != 0)\n",
      "    clf_miss = t_train[(clf_preds - t_train) != 0]\n",
      "    \n",
      "    rate = 1 - np.mean(clf_missId) #error rate\n",
      "    return rate, clf_miss\n",
      "\n",
      "def name_for_feature(command, t_train, X_train, feature_dict):\n",
      "    try:\n",
      "        col_id = feature_dict[command]\n",
      "    except KeyError:\n",
      "        print (\"command not exist\")\n",
      "        raise KeyError\n",
      "        \n",
      "    t_train = t_train.reshape(1, np.max(t_train.shape))\n",
      "    ftData = X_train.T[col_id]\n",
      "\n",
      "    nmIdx = t_train[np.array(ftData>0)]\n",
      "    out = [util.malware_classes[i] for i in nmIdx]\n",
      "    out = stats.itemfreq(out)\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def pruneFeatures(minFreq, X_train_dense, global_feat_dict):\n",
      "    print \"pruning training features...\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    #featureFreq = [sum(feature != 0) for feature in X_train_dense.T]\n",
      "    #featureFreq_pd = pd.Series(featureFreq, global_feat_dict.keys())\n",
      "    featureFreq = []\n",
      "    X_train_denseT = X_train_dense.T\n",
      "    i = 0\n",
      "    i_max = len(X_train_denseT)\n",
      "\n",
      "    for feature in X_train_denseT:\n",
      "        featureFreq.append(sum(feature != 0))\n",
      "        i += 1\n",
      "        if i % 5000 == 0: \n",
      "            print \"Obtaining featureFreq: \" + \\\n",
      "                    str(round(float(i)*100/i_max, 3)) + \"%\"\n",
      "\n",
      "    print \"obtaining prunId...\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    prunId = []\n",
      "    i = 0\n",
      "    for item in featureFreq: \n",
      "        if item > minFreq: prunId.append(i)\n",
      "        i += 1\n",
      "        if i % 5000 == 0: \n",
      "            print \"Obtaining prunId: \" + \\\n",
      "                    str(round(float(i)*100/i_max, 3)) + \"%\"\n",
      "            sys.stdout.flush()\n",
      "        \n",
      "    print \"obtaining prunId...Done!\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    print \"Pruning Design Matrix... \"; sys.stdout.flush()\n",
      "    X_train_prune = X_train_dense.T[prunId].T #update X\n",
      "    print \"Pruning Design Matrix...Done!\"\n",
      "\n",
      "    global_feat_dict_prune = Counter() #update global_feat_dict\n",
      "    \n",
      "    dictList = global_feat_dict.items()\n",
      "    dictList_prune = np.array(dictList)[np.array(prunId)]\n",
      "    \n",
      "    for i in range(len(dictList_prune)):\n",
      "        global_feat_dict_prune[dictList_prune[i][0]] = dictList_prune[i][1]\n",
      "        if i % 5000 == 0: \n",
      "            print \"Pruning feature dict: \" + \\\n",
      "                str(round(float(i)*100/len(dictList_prune), 3)) + \"%\"\n",
      "            sys.stdout.flush()\n",
      "\n",
      "    print \"done pruning training features!\"\n",
      "    return X_train_prune, global_feat_dict_prune, featureFreq, prunId\n",
      "\n",
      "    \n",
      "################################################\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "################################\n",
      "#### 2. IO functions\n",
      "################################\n",
      "\n",
      "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      ffs are a list of feature-functions.\n",
      "      direc is a directory containing xml files (expected to be train or test).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "\n",
      "    returns: \n",
      "      a sparse design matrix, a dict mapping features to column-numbers,\n",
      "      a vector of target classes, and a list of system-call-history ids in order \n",
      "      of their rows in the design matrix.\n",
      "      \n",
      "      Note: the vector of target classes returned will contain the true indices of the\n",
      "      target classes on the training data, but will contain only -1's on the test\n",
      "      data\n",
      "    \"\"\"\n",
      "    \n",
      "    fds = [] # list of feature dicts\n",
      "    classes = []\n",
      "    ids = [] \n",
      "    \n",
      "    i = 0\n",
      "    N = str(len(os.listdir(direc)))\n",
      "    \n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        ids.append(id_str)\n",
      "        # add target class if this is training data\n",
      "        '''\n",
      "        try:\n",
      "            classes.append(util.malware_classes.index(clazz))\n",
      "        except ValueError:\n",
      "            # we should only fail to find the label in our list of malware classes\n",
      "            # if this is test data, which always has an \"X\" label\n",
      "            assert clazz == \"XY\"\n",
      "            classes.append(-1)\n",
      "        '''\n",
      "        rowfd = {}\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        [rowfd.update(ff(tree)) for ff in ffs]\n",
      "        fds.append(rowfd)\n",
      "        \n",
      "        i += 1\n",
      "        if i%100 == 0: \n",
      "            print \"Progress: \" + str(i) + \"/\" + N\n",
      "            sys.stdout.flush()\n",
      "        \n",
      "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
      "    return X, feat_dict, np.array(classes), ids\n",
      "\n",
      "\n",
      "def make_design_mat(fds, global_feat_dict = None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      fds is a list of feature dicts (one for each row).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "       \n",
      "    returns: \n",
      "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
      "        the union of features defined in any of the fds \n",
      "    \"\"\"\n",
      "    if global_feat_dict is None:\n",
      "        all_feats = set()\n",
      "        [all_feats.update(fd.keys()) for fd in fds]\n",
      "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
      "    else:\n",
      "        feat_dict = global_feat_dict\n",
      "        \n",
      "    cols = []\n",
      "    rows = []\n",
      "    data = []        \n",
      "    for i in xrange(len(fds)):\n",
      "        temp_cols = []\n",
      "        temp_data = []\n",
      "        for feat,val in fds[i].iteritems():\n",
      "            try:\n",
      "                # update temp_cols iff update temp_data\n",
      "                temp_cols.append(feat_dict[feat])\n",
      "                temp_data.append(val)\n",
      "            except KeyError as ex:\n",
      "                if global_feat_dict is not None:\n",
      "                    pass  # new feature in test data; nbd\n",
      "                else:\n",
      "                    raise ex\n",
      "\n",
      "        # all fd's features in the same row\n",
      "        k = len(temp_cols)\n",
      "        cols.extend(temp_cols)\n",
      "        data.extend(temp_data)\n",
      "        rows.extend([i]*k)\n",
      "\n",
      "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
      "   \n",
      "\n",
      "    X = sparse.csr_matrix(\n",
      "    (np.array(data),(np.array(rows), np.array(cols))), \n",
      "    shape=(len(fds), len(feat_dict))\n",
      "    )\n",
      "    \n",
      "    return X, feat_dict\n",
      "    \n",
      "################################################"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "################################\n",
      "#### 3. Feature Vectors\n",
      "################################\n",
      "\n",
      "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
      "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
      "# feature-names to numeric values.\n",
      "## TODO: \n",
      "##    1. DLL Type and Address.\n",
      "##    2. enum_values, explorer\n",
      "##    3. create_namedpipe\n",
      "##    4. create_thread_remote\n",
      "\n",
      "\n",
      "def get_all_keys(tree):\n",
      "    key_list = Counter()\n",
      "    tag_exclusion_set = [\"load_dll\"]\n",
      "    \n",
      "    in_all_section = False\n",
      "\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            #if el.tag in tag_set\n",
      "            for key in el.keys():\n",
      "                if key not in tag_exclusion_set:\n",
      "                    key_list_name = \\\n",
      "                        (el.tag + \"_\" + key + \"-\" + el.get(key)).lower()\n",
      "                    key_list[key_list_name] += 1\n",
      "    return key_list\n",
      "\n",
      "\n",
      "def dll_type(tree, name_only = False):\n",
      "    #type and address of loaded DLLs\n",
      "    dll_list = {\"name\": [], \"extn\": [], \"addr\": []} \n",
      "\n",
      "    for el in tree.iter():\n",
      "        # obtain DLL target in element\n",
      "        if el.tag == \"load_dll\":\n",
      "            dll_name = el.get('filename')\n",
      "            #split filename and file_address\n",
      "            try:\n",
      "                key_bag = dll_name.split(\"\\\\\")\n",
      "                dll_name = \"dll_name-\" + key_bag[len(key_bag) - 1]\n",
      "                dll_addr = \"dll_addr-\" + \"//\".join(key_bag[:(len(key_bag) - 1)])\n",
      "            except AttributeError:\n",
      "                dll_name = \"dll_name-DLL_FILE_NOT_SPECIFIED\"\n",
      "                dll_addr = \"dll_addr-\" + \"//\".join(key_bag)\n",
      "            \n",
      "            if len(dll_name.split(\".\")) == 2:\n",
      "                key_bag = dll_name.split(\".\")\n",
      "                dll_name = key_bag[0]\n",
      "                dll_extn = \"dll_extn-\" + key_bag[1]\n",
      "            else:\n",
      "                dll_extn = \"dll_extn-\"\n",
      "            \n",
      "            dll_name = dll_name.lower()\n",
      "            dll_extn = dll_extn.lower()\n",
      "            dll_addr = dll_addr.lower()\n",
      "            #TODO: convert to lower case\n",
      "            dll_list[\"name\"].append(dll_name)\n",
      "            dll_list[\"extn\"].append(dll_extn)\n",
      "            dll_list[\"addr\"].append(dll_addr)\n",
      "\n",
      "    dll_list[\"name\"] = stats.itemfreq(dll_list[\"name\"])\n",
      "    dll_list[\"extn\"] = stats.itemfreq(dll_list[\"extn\"])    \n",
      "    dll_list[\"addr\"] = stats.itemfreq(dll_list[\"addr\"])    \n",
      "    dll_list_join = np.concatenate([dll_list[\"name\"],dll_list[\"extn\"], dll_list[\"addr\"]])\n",
      "\n",
      "    dll_name_counter = Counter()\n",
      "    for item in dll_list_join: \n",
      "        dll_name_counter[item[0]] = int(item[1])\n",
      "\n",
      "    return dll_name_counter\n",
      "\n",
      "\n",
      "def call_freq(tree, name_only = False):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    callz = []\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            callz.append(el.tag)\n",
      "\n",
      "    # finally, count the frequencies\n",
      "    freqList = stats.itemfreq(callz)   \n",
      "    \n",
      "    if name_only == True:        \n",
      "        c = set(callz)\n",
      "    else: \n",
      "        c = Counter()\n",
      "        for item in freqList: c[\"sys_call-\" +item[0]] = int(item[1])\n",
      "\n",
      "    return c\n",
      "\n",
      "\n",
      "def call_eigen(tree, call_dict, name_only = False):\n",
      "    #TODO: Implement this!\n",
      "    c = []\n",
      "    return c\n",
      "\n",
      "\n",
      "def first_last_system_call_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            if first:\n",
      "                c[\"first_call-\"+el.tag] = 1\n",
      "                first = False\n",
      "            last_call = el.tag  # update last call seen\n",
      "            \n",
      "    # finally, mark last call seen\n",
      "    c[\"last_call-\"+last_call] = 1\n",
      "    return c\n",
      "\n",
      "def system_call_count_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
      "      made by an executable (summed over all processes)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c['num_system_calls'] += 1\n",
      "    return c\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "################################\n",
      "#### 4. Variables of Interest\n",
      "################################\n",
      "\n",
      "\n",
      "def call_type(direc):\n",
      "    names = set() # list of feature dicts\n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        newnames = call_freq(tree, name_only = True)\n",
      "        names = names.union(newnames)\n",
      "    out_names = names\n",
      "    return out_names\n",
      "\n",
      "def call_freq_emp(direc):\n",
      "    names = dict() # list of feature dicts\n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        newnames = call_freq(tree)\n",
      "        for k in newnames.iterkeys():\n",
      "            if k in names: names[k] = names[k] + newnames[k]\n",
      "            else: names[k] = newnames[k]\n",
      "            \n",
      "    out_names = names\n",
      "    return out_names\n",
      "    \n",
      "def call_freq_byType(direc):\n",
      "    #initiate class dependent dictionary\n",
      "    name_byType = dict() # list of feature dicts\n",
      "    for TypeName in util.malware_classes:\n",
      "        name_byType[TypeName] = dict()\n",
      " \n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features: first create new feature\n",
      "        newnames = call_freq(tree)\n",
      "        # \n",
      "        curNames = name_byType[clazz]\n",
      "        for k in newnames.iterkeys():\n",
      "            if k in curNames: \n",
      "                curNames[k] = curNames[k] + newnames[k]\n",
      "            else: curNames[k] = newnames[k]\n",
      "        name_byType[clazz] = curNames\n",
      "        \n",
      "    out_names = name_byType\n",
      "    return out_names\n",
      "    \n",
      "def dll_type_2(tree, name_only = False):\n",
      "    dll_list = {\"name\": [], \"addr\": []} \n",
      "\n",
      "    for el in tree.iter():\n",
      "        # obtain DLL target in element\n",
      "        if el.tag == \"load_dll\":\n",
      "            dll_name = el.get('filename')\n",
      "            #split filename and file_address\n",
      "            key_bag = dll_name.split(\"\\\\\")\n",
      "            \n",
      "            dll_name = key_bag[len(key_bag) - 1]\n",
      "            dll_addr = \"//\".join(key_bag[:(len(key_bag) - 1)])\n",
      "            #TODO: convert to lower case\n",
      "\n",
      "            dll_list[\"name\"].append(dll_name)\n",
      "            dll_list[\"addr\"].append(dll_addr)\n",
      "\n",
      "    dll_list[\"name\"] = stats.itemfreq(dll_list[\"name\"])\n",
      "    dll_list[\"addr\"] = stats.itemfreq(dll_list[\"addr\"])    \n",
      "    dll_list_join = np.concatenate([dll_list[\"name\"], dll_list[\"addr\"]])\n",
      "\n",
      "    dll_name_counter = Counter()\n",
      "    for item in dll_list_join: \n",
      "        dll_name_counter[item[0]] = int(item[1])\n",
      "\n",
      "    return dll_name_counter\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def call_eigen(tree, call_dict, name_only = False):\n",
      "    #TODO: Implement this!\n",
      "    c = []\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_dir = \"train\"\n",
      "test_dir=\"test\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ffs = [first_last_system_call_feats, system_call_count_feats,call_freq, dll_type]#, get_all_keys]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ffs = [first_last_system_call_feats, system_call_count_feats]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 1900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2000/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2100/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2200/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2300/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2400/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2500/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2600/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2700/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2800/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 2900/3086\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Progress: 3000/3086\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "array([], dtype=float64)"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_dense"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
        "        [4, 0, 0, ..., 0, 0, 0],\n",
        "        [0, 0, 0, ..., 0, 0, 0],\n",
        "        ..., \n",
        "        [0, 0, 0, ..., 0, 0, 0],\n",
        "        [0, 0, 0, ..., 0, 0, 0],\n",
        "        [7, 0, 0, ..., 0, 0, 0]])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_dir=\"test\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_dense = X_train.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(0,)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "x = range(50, 80, 2)\n",
      "y = []\n",
      "\n",
      "for i in x:\n",
      "    clf4 = RandomForestClassifier(n_estimators=i, criterion=\"entropy\")\n",
      "    clf4.fit(X_train.toarray(), t_train)\n",
      "    y_pred4 = clf4.predict(X_train.toarray())\n",
      "\n",
      "    y.append((t_train != y_pred4).sum())\n",
      "\n",
      "z = np.polyfit(x, y, 15)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "x_new = x\n",
      "y_new = f(x_new)\n",
      "\n",
      "plt.plot(x,y,'x', x_new, y_new)\n",
      "\n",
      "plt.ylabel(\"Number of Errors\", fontsize=18)\n",
      "plt.xlabel(\"Number of max_features\", fontsize=18)\n",
      "plt.title(\"Number of miss-matches in the Training Set - ENTROPY\", fontsize=20);\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Number of labels=0 does not match number of samples=3086",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-30-57c6cc2a2c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0my_pred4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 279\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \"\"\"\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(trees, forest, X, y, sample_weight, verbose)\u001b[0m\n\u001b[1;32m     87\u001b[0m             tree.fit(X, y,\n\u001b[1;32m     88\u001b[0m                      \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                      check_input=False)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_mask, X_argsorted, check_input, sample_weight)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 206\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_samples_split must be greater than zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Number of labels=0 does not match number of samples=3086"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "x = range(50, 80, 2)\n",
      "y = []\n",
      "\n",
      "for i in x:\n",
      "    clf4 = RandomForestClassifier(n_estimators=i, criterion=\"gini\")\n",
      "    y_pred4 = clf4.fit(X_train.toarray(), t_train).predict(X_train.toarray())\n",
      "\n",
      "    y.append((t_train != y_pred4).sum())\n",
      "\n",
      "z = np.polyfit(x, y, 15)\n",
      "f = np.poly1d(z)\n",
      "\n",
      "x_new = x\n",
      "y_new = f(x_new)\n",
      "\n",
      "plt.plot(x,y,'x', x_new, y_new)\n",
      "\n",
      "plt.ylabel(\"Number of Errors\", fontsize=18)\n",
      "plt.xlabel(\"Number of max_features\", fontsize=18)\n",
      "plt.title(\"Number of miss-matches in the Training Set - ENTROPY\", fontsize=20);\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Number of labels=0 does not match number of samples=3086",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-21-b2cb9f450353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gini\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0my_pred4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_train\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 279\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \"\"\"\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(trees, forest, X, y, sample_weight, verbose)\u001b[0m\n\u001b[1;32m     87\u001b[0m             tree.fit(X, y,\n\u001b[1;32m     88\u001b[0m                      \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                      check_input=False)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/vivian/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_mask, X_argsorted, check_input, sample_weight)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 206\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_samples_split must be greater than zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Number of labels=0 does not match number of samples=3086"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}