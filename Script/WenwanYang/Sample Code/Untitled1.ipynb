{
 "metadata": {
  "name": "",
  "signature": "sha256:fea7b86763232e4db2003301dfc289727d1b9ddf86636ccc53b3345257acbd28"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import os\n",
      "import sys\n",
      "from collections import Counter\n",
      "try:\n",
      "    import xml.etree.cElementTree as ET\n",
      "except ImportError:\n",
      "    import xml.etree.ElementTree as ET\n",
      "\n",
      "#bread and butter\n",
      "import nltk\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "from scipy import sparse\n",
      "\n",
      "#toyz\n",
      "from sklearn import cross_validation as cv\n",
      "from sklearn import ensemble as es\n",
      "os.chdir(\"/Users/Vivian/Desktop/data\")\n",
      "import util\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "################################################\n",
      "#### 1. Auxilliary Funcs #######################\n",
      "################################################\n",
      "def print_full(x):\n",
      "    #print all rows of a panda dataframe\n",
      "    pd.set_option('display.max_rows', len(x))\n",
      "    print(x)\n",
      "    pd.reset_option('display.max_rows')\n",
      "\n",
      "\n",
      "def Accuracy(clf_preds, t_train):\n",
      "    clf_missId = ((clf_preds - t_train) != 0)\n",
      "    clf_miss = t_train[(clf_preds - t_train) != 0]\n",
      "    \n",
      "    rate = 1 - np.mean(clf_missId) #error rate\n",
      "    return rate, clf_miss\n",
      "\n",
      "def name_for_feature(command, t_train, X_train, feature_dict):\n",
      "    try:\n",
      "        col_id = feature_dict[command]\n",
      "    except KeyError:\n",
      "        print (\"command not exist\")\n",
      "        raise KeyError\n",
      "        \n",
      "    t_train = t_train.reshape(1, np.max(t_train.shape))\n",
      "    ftData = X_train.T[col_id]\n",
      "\n",
      "    nmIdx = t_train[np.array(ftData>0)]\n",
      "    out = [util.malware_classes[i] for i in nmIdx]\n",
      "    out = stats.itemfreq(out)\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def pruneFeatures(minFreq, X_train_dense, global_feat_dict):\n",
      "    print \"pruning training features...\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    #featureFreq = [sum(feature != 0) for feature in X_train_dense.T]\n",
      "    #featureFreq_pd = pd.Series(featureFreq, global_feat_dict.keys())\n",
      "    featureFreq = []\n",
      "    X_train_denseT = X_train_dense.T\n",
      "    i = 0\n",
      "    i_max = len(X_train_denseT)\n",
      "\n",
      "    for feature in X_train_denseT:\n",
      "        featureFreq.append(sum(feature != 0))\n",
      "        i += 1\n",
      "        if i % 5000 == 0: \n",
      "            print \"Obtaining featureFreq: \" + \\\n",
      "                    str(round(float(i)*100/i_max, 3)) + \"%\"\n",
      "\n",
      "    print \"obtaining prunId...\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    prunId = []\n",
      "    i = 0\n",
      "    for item in featureFreq: \n",
      "        if item > minFreq: prunId.append(i)\n",
      "        i += 1\n",
      "        if i % 5000 == 0: \n",
      "            print \"Obtaining prunId: \" + \\\n",
      "                    str(round(float(i)*100/i_max, 3)) + \"%\"\n",
      "            sys.stdout.flush()\n",
      "        \n",
      "    print \"obtaining prunId...Done!\"\n",
      "    sys.stdout.flush()\n",
      "\n",
      "    print \"Pruning Design Matrix... \"; sys.stdout.flush()\n",
      "    X_train_prune = X_train_dense.T[prunId].T #update X\n",
      "    print \"Pruning Design Matrix...Done!\"\n",
      "\n",
      "    global_feat_dict_prune = Counter() #update global_feat_dict\n",
      "    \n",
      "    dictList = global_feat_dict.items()\n",
      "    dictList_prune = np.array(dictList)[np.array(prunId)]\n",
      "    \n",
      "    for i in range(len(dictList_prune)):\n",
      "        global_feat_dict_prune[dictList_prune[i][0]] = dictList_prune[i][1]\n",
      "        if i % 5000 == 0: \n",
      "            print \"Pruning feature dict: \" + \\\n",
      "                str(round(float(i)*100/len(dictList_prune), 3)) + \"%\"\n",
      "            sys.stdout.flush()\n",
      "\n",
      "    print \"done pruning training features!\"\n",
      "    return X_train_prune, global_feat_dict_prune, featureFreq, prunId\n",
      "\n",
      "    \n",
      "################################################\n",
      "\n",
      "################################\n",
      "#### 2. IO functions\n",
      "################################\n",
      "\n",
      "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      ffs are a list of feature-functions.\n",
      "      direc is a directory containing xml files (expected to be train or test).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "\n",
      "    returns: \n",
      "      a sparse design matrix, a dict mapping features to column-numbers,\n",
      "      a vector of target classes, and a list of system-call-history ids in order \n",
      "      of their rows in the design matrix.\n",
      "      \n",
      "      Note: the vector of target classes returned will contain the true indices of the\n",
      "      target classes on the training data, but will contain only -1's on the test\n",
      "      data\n",
      "    \"\"\"\n",
      "    \n",
      "    fds = [] # list of feature dicts\n",
      "    classes = []\n",
      "    ids = [] \n",
      "    \n",
      "    i = 0\n",
      "    N = str(len(os.listdir(direc)))\n",
      "    \n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        ids.append(id_str)\n",
      "        # add target class if this is training data\n",
      "        try:\n",
      "            classes.append(util.malware_classes.index(clazz))\n",
      "        except ValueError:\n",
      "            # we should only fail to find the label in our list of malware classes\n",
      "            # if this is test data, which always has an \"X\" label\n",
      "            assert clazz == \"X\"\n",
      "            classes.append(-1)\n",
      "        rowfd = {}\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        [rowfd.update(ff(tree)) for ff in ffs]\n",
      "        fds.append(rowfd)\n",
      "        \n",
      "        i += 1\n",
      "        if i%100 == 0: \n",
      "            print \"Progress: \" + str(i) + \"/\" + N\n",
      "            sys.stdout.flush()\n",
      "        \n",
      "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
      "    return X, feat_dict, np.array(classes), ids\n",
      "\n",
      "\n",
      "def make_design_mat(fds, global_feat_dict = None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      fds is a list of feature dicts (one for each row).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "       \n",
      "    returns: \n",
      "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
      "        the union of features defined in any of the fds \n",
      "    \"\"\"\n",
      "    if global_feat_dict is None:\n",
      "        all_feats = set()\n",
      "        [all_feats.update(fd.keys()) for fd in fds]\n",
      "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
      "    else:\n",
      "        feat_dict = global_feat_dict\n",
      "        \n",
      "    cols = []\n",
      "    rows = []\n",
      "    data = []        \n",
      "    for i in xrange(len(fds)):\n",
      "        temp_cols = []\n",
      "        temp_data = []\n",
      "        for feat,val in fds[i].iteritems():\n",
      "            try:\n",
      "                # update temp_cols iff update temp_data\n",
      "                temp_cols.append(feat_dict[feat])\n",
      "                temp_data.append(val)\n",
      "            except KeyError as ex:\n",
      "                if global_feat_dict is not None:\n",
      "                    pass  # new feature in test data; nbd\n",
      "                else:\n",
      "                    raise ex\n",
      "\n",
      "        # all fd's features in the same row\n",
      "        k = len(temp_cols)\n",
      "        cols.extend(temp_cols)\n",
      "        data.extend(temp_data)\n",
      "        rows.extend([i]*k)\n",
      "\n",
      "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
      "   \n",
      "\n",
      "    X = sparse.csr_matrix(\n",
      "    (np.array(data),(np.array(rows), np.array(cols))), \n",
      "    shape=(len(fds), len(feat_dict))\n",
      "    )\n",
      "    \n",
      "    return X, feat_dict\n",
      "    \n",
      "################################################\n",
      "\n",
      "################################\n",
      "#### 3. Feature Vectors\n",
      "################################\n",
      "\n",
      "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
      "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
      "# feature-names to numeric values.\n",
      "## TODO: \n",
      "##    1. DLL Type and Address.\n",
      "##    2. enum_values, explorer\n",
      "##    3. create_namedpipe\n",
      "##    4. create_thread_remote\n",
      "\n",
      "\n",
      "def get_all_keys(tree):\n",
      "    key_list = Counter()\n",
      "    tag_exclusion_set = [\"load_dll\"]\n",
      "    \n",
      "    in_all_section = False\n",
      "\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            #if el.tag in tag_set\n",
      "            for key in el.keys():\n",
      "                if key not in tag_exclusion_set:\n",
      "                    key_list_name = \\\n",
      "                        (el.tag + \"_\" + key + \"-\" + el.get(key)).lower()\n",
      "                    key_list[key_list_name] += 1\n",
      "    return key_list\n",
      "\n",
      "\n",
      "def dll_type(tree, name_only = False):\n",
      "    #type and address of loaded DLLs\n",
      "    dll_list = {\"name\": [], \"extn\": [], \"addr\": []} \n",
      "\n",
      "    for el in tree.iter():\n",
      "        # obtain DLL target in element\n",
      "        if el.tag == \"load_dll\":\n",
      "            dll_name = el.get('filename')\n",
      "            #split filename and file_address\n",
      "            try:\n",
      "                key_bag = dll_name.split(\"\\\\\")\n",
      "                dll_name = \"dll_name-\" + key_bag[len(key_bag) - 1]\n",
      "                dll_addr = \"dll_addr-\" + \"//\".join(key_bag[:(len(key_bag) - 1)])\n",
      "            except AttributeError:\n",
      "                dll_name = \"dll_name-DLL_FILE_NOT_SPECIFIED\"\n",
      "                dll_addr = \"dll_addr-\" + \"//\".join(key_bag)\n",
      "            \n",
      "            if len(dll_name.split(\".\")) == 2:\n",
      "                key_bag = dll_name.split(\".\")\n",
      "                dll_name = key_bag[0]\n",
      "                dll_extn = \"dll_extn-\" + key_bag[1]\n",
      "            else:\n",
      "                dll_extn = \"dll_extn-\"\n",
      "            \n",
      "            dll_name = dll_name.lower()\n",
      "            dll_extn = dll_extn.lower()\n",
      "            dll_addr = dll_addr.lower()\n",
      "            #TODO: convert to lower case\n",
      "            dll_list[\"name\"].append(dll_name)\n",
      "            dll_list[\"extn\"].append(dll_extn)\n",
      "            dll_list[\"addr\"].append(dll_addr)\n",
      "\n",
      "    dll_list[\"name\"] = stats.itemfreq(dll_list[\"name\"])\n",
      "    dll_list[\"extn\"] = stats.itemfreq(dll_list[\"extn\"])    \n",
      "    dll_list[\"addr\"] = stats.itemfreq(dll_list[\"addr\"])    \n",
      "    dll_list_join = concatenate([dll_list[\"name\"], \\\n",
      "                    dll_list[\"extn\"], dll_list[\"addr\"]])\n",
      "\n",
      "    dll_name_counter = Counter()\n",
      "    for item in dll_list_join: \n",
      "        dll_name_counter[item[0]] = int(item[1])\n",
      "\n",
      "    return dll_name_counter\n",
      "\n",
      "\n",
      "def call_freq(tree, name_only = False):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    callz = []\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            callz.append(el.tag)\n",
      "\n",
      "    # finally, count the frequencies\n",
      "    freqList = stats.itemfreq(callz)   \n",
      "    \n",
      "    if name_only == True:        \n",
      "        c = set(callz)\n",
      "    else: \n",
      "        c = Counter()\n",
      "        for item in freqList: c[\"sys_call-\" +item[0]] = int(item[1])\n",
      "\n",
      "    return c\n",
      "\n",
      "\n",
      "def call_eigen(tree, call_dict, name_only = False):\n",
      "    #TODO: Implement this!\n",
      "    c = []\n",
      "    return c\n",
      "\n",
      "\n",
      "def first_last_system_call_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            if first:\n",
      "                c[\"first_call-\"+el.tag] = 1\n",
      "                first = False\n",
      "            last_call = el.tag  # update last call seen\n",
      "            \n",
      "    # finally, mark last call seen\n",
      "    c[\"last_call-\"+last_call] = 1\n",
      "    return c\n",
      "\n",
      "def system_call_count_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
      "      made by an executable (summed over all processes)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c['num_system_calls'] += 1\n",
      "    return c\n",
      "\n",
      "\n",
      "################################\n",
      "#### 4. Variables of Interest\n",
      "################################\n",
      "\n",
      "\n",
      "def call_type(direc):\n",
      "    names = set() # list of feature dicts\n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        newnames = call_freq(tree, name_only = True)\n",
      "        names = names.union(newnames)\n",
      "    out_names = names\n",
      "    return out_names\n",
      "\n",
      "def call_freq_emp(direc):\n",
      "    names = dict() # list of feature dicts\n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        newnames = call_freq(tree)\n",
      "        for k in newnames.iterkeys():\n",
      "            if k in names: names[k] = names[k] + newnames[k]\n",
      "            else: names[k] = newnames[k]\n",
      "            \n",
      "    out_names = names\n",
      "    return out_names\n",
      "    \n",
      "def call_freq_byType(direc):\n",
      "    #initiate class dependent dictionary\n",
      "    name_byType = dict() # list of feature dicts\n",
      "    for TypeName in util.malware_classes:\n",
      "        name_byType[TypeName] = dict()\n",
      " \n",
      "    for datafile in os.listdir(direc):\n",
      "        if datafile == \"DS.Store\": continue\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features: first create new feature\n",
      "        newnames = call_freq(tree)\n",
      "        # \n",
      "        curNames = name_byType[clazz]\n",
      "        for k in newnames.iterkeys():\n",
      "            if k in curNames: \n",
      "                curNames[k] = curNames[k] + newnames[k]\n",
      "            else: curNames[k] = newnames[k]\n",
      "        name_byType[clazz] = curNames\n",
      "        \n",
      "    out_names = name_byType\n",
      "    return out_names\n",
      "    \n",
      "def dll_type_2(tree, name_only = False):\n",
      "    dll_list = {\"name\": [], \"addr\": []} \n",
      "\n",
      "    for el in tree.iter():\n",
      "        # obtain DLL target in element\n",
      "        if el.tag == \"load_dll\":\n",
      "            dll_name = el.get('filename')\n",
      "            #split filename and file_address\n",
      "            key_bag = dll_name.split(\"\\\\\")\n",
      "            \n",
      "            dll_name = key_bag[len(key_bag) - 1]\n",
      "            dll_addr = \"//\".join(key_bag[:(len(key_bag) - 1)])\n",
      "            #TODO: convert to lower case\n",
      "\n",
      "            dll_list[\"name\"].append(dll_name)\n",
      "            dll_list[\"addr\"].append(dll_addr)\n",
      "\n",
      "    dll_list[\"name\"] = stats.itemfreq(dll_list[\"name\"])\n",
      "    dll_list[\"addr\"] = stats.itemfreq(dll_list[\"addr\"])    \n",
      "    dll_list_join = concatenate([dll_list[\"name\"], dll_list[\"addr\"]])\n",
      "\n",
      "    dll_name_counter = Counter()\n",
      "    for item in dll_list_join: \n",
      "        dll_name_counter[item[0]] = int(item[1])\n",
      "\n",
      "    return dll_name_counter\n",
      "\n",
      "\n",
      "## The following function does the feature extraction, learning, and prediction\n",
      "def main():\n",
      "    train_dir = \"../../../Data/train\"\n",
      "    test_dir = \"../../../Data/test\"\n",
      "    outputfile = \"../../Output/Jeremiah.csv\"  # feel free to change this or take it as an argument\n",
      " \n",
      "    ################################\n",
      "    #### Empirical Summary \n",
      "    ################################\n",
      "\n",
      "    #Get types & fre of commands\n",
      "    \n",
      "    #raw count\n",
      "    lesNames =  call_freq(train_dir)\n",
      "    lesNames_freq = pd.Series(lesNames.values(), lesNames.keys())\n",
      "    lesNames_freq = lesNames_freq/sum(lesNames_freq)\n",
      "    lesNames_freq.sort()\n",
      "    lesNames_freq\n",
      "\n",
      "    #raw count by Type\n",
      "    lesNames_byType =  call_freq_byType(train_dir)\n",
      "    \n",
      "    lesNames_byType_freq = dict()\n",
      "    for TypeName in util.malware_classes:\n",
      "        lesNames_byType_freq[TypeName] = []\n",
      "    \n",
      "    for keyName in lesNames_byType.keys():\n",
      "        namez = lesNames_byType[keyName]\n",
      "        namez_freq = pd.Series(namez.values(), namez.keys())\n",
      "        namez_freq = namez_freq/sum(namez_freq)\n",
      "        namez_freq.sort(ascending = False)\n",
      "        lesNames_byType_freq[keyName] = namez_freq[namez_freq > 0.01]\n",
      "        print(keyName + \" Finished!\")\n",
      "        sys.stdout.flush()\n",
      "    \n",
      "    lesNames_byType_freq #most frequent commands in each class\n",
      "    \n",
      "       \n",
      "    # TODO put the names of the feature functions you've defined above in this list\n",
      "    ffs = [first_last_system_call_feats, system_call_count_feats, \\\n",
      "            call_freq, dll_type]#, get_all_keys]\n",
      "    \n",
      "    ################################\n",
      "    #### Feature Extraction and Prunning \n",
      "    ################################\n",
      "\n",
      "    # extract features\n",
      "    print \"extracting training features...\"\n",
      "    X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
      "    X_train_dense = X_train.todense()\n",
      "    del X_train\n",
      "    print \"done extracting training features\"\n",
      "    print\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "    #prunning\n",
      "    X_train_prune, global_feat_dict_prune, featureFreq, prunId = \\\n",
      "    pruneFeatures(minFreq = 1, X_train_dense = X_train_dense, \\\n",
      "                    global_feat_dict = global_feat_dict)\n",
      "\n",
      "    ################################\n",
      "    #### CV-based training \n",
      "    ################################\n",
      "    n = X_train_dense.shape[0]\n",
      "    nForest = 1000\n",
      "    n_cv = 5\n",
      "    \n",
      "    print str(n_cv) + \" fold learning initiated...\"\n",
      "    eRate_cv = []\n",
      "    kf_cv = cv.KFold(n, n_folds = n_cv)\n",
      "    clf_cv = es.RandomForestClassifier(n_estimators = nForest)\n",
      "    i = 0\n",
      "    \n",
      "    for train_index, test_index in kf_cv:\n",
      "        i += 1\n",
      "        #create CV dataset and fit\n",
      "        F_train, F_test = X_train_prune[train_index], X_train_prune[test_index]\n",
      "        y_train, y_test = t_train[train_index], t_train[test_index]\n",
      "        clf_fit = clf_cv.fit(F_train, y_train)\n",
      "        #prediction\n",
      "        clf_pred = clf_fit.predict(F_test)\n",
      "        accuracy = Accuracy(clf_pred, y_test)[0]\n",
      "        eRate_cv.append(accuracy)\n",
      "        print(\"Fold \" + str(i) + \" Classification Accuracy = \" + str(accuracy))\n",
      "        sys.stdout.flush()\n",
      "    print \"done learning\"\n",
      "    print\n",
      "    \n",
      "    np.mean(eRate_cv)\n",
      "\n",
      "    ################################\n",
      "    #feature importance assessment: \n",
      "    ################################\n",
      "    # train here, and learn your classification parameters\n",
      "    print \"learning...\"\n",
      "    nForest = 1000\n",
      "    clf = es.RandomForestClassifier(n_estimators = nForest, \\\n",
      "                verbose = 1, n_jobs = -1)\n",
      "    clf_fit = clf.fit(X_train_dense, t_train)\n",
      "    print \"done learning\"\n",
      "    print\n",
      "\n",
      "    #TODO: Figure out param Name that Feature Importance corresponds to\n",
      "    ftImp = pd.DataFrame(sorted(global_feat_dict.keys()), \\\n",
      "                            columns = [\"Name\"])\n",
      "    ftImp[\"FeatureImp\"] = clf_fit.feature_importances_\n",
      "    ftImp_s = ftImp.sort(columns = \"FeatureImp\", ascending = False)\n",
      "\n",
      "    print_full(ftImp_s)\n",
      "    ftImp_s.loc[ ftImp_s['FeatureImp']> 0.000, :]\n",
      "\n",
      "    ####################################\n",
      "    # in sample prediction and mis-classification rate\n",
      "    ####################################\n",
      "    \n",
      "    print \"making in-sample predictions...\"\n",
      "    clf_preds = clf_fit.predict(X_train_dense)\n",
      "    clf_missId = ((clf_preds - t_train) != 0)\n",
      "    clf_miss = t_train[(clf_preds - t_train) != 0]\n",
      "    \n",
      "    rate = 1 - np.mean(clf_missId) #error rate\n",
      "    \n",
      "    clf_miss = [util.malware_classes[i] for i in clf_miss]\n",
      "    stats.itemfreq(clf_miss)\n",
      "    print \"done making in-sample predictions\"\n",
      "    \n",
      "    # get rid of training data and load test data\n",
      "    del X_train\n",
      "    del t_train\n",
      "    del train_ids\n",
      "    print \"extracting test features...\"\n",
      "    X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
      "    X_test_dense = X_test.todense()\n",
      "    X_test_prune = X_test_dense.T[prunId].T\n",
      "    \n",
      "    print \"done extracting test features\"\n",
      "    print\n",
      "    \n",
      "    # TODO make predictions on text data and write them out\n",
      "    print \"making predictions...\"\n",
      "    clf_preds = clf_fit.predict(X_test_prune)\n",
      "    print \"done making predictions\"\n",
      "    print\n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(clf_preds, test_ids, outputfile)\n",
      "    print \"done!\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'str' object has no attribute 'iter'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-a8924f4fabc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-1-a8924f4fabc6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;31m#raw count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0mlesNames\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcall_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0mlesNames_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlesNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlesNames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0mlesNames_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlesNames_freq\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlesNames_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-1-a8924f4fabc6>\u001b[0m in \u001b[0;36mcall_freq\u001b[0;34m(tree, name_only)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m \u001b[0;31m# is this the first system call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mlast_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;31m# keep track of last call we've seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;31m# ignore everything outside the \"all_section\" element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all_section\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_all_section\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'iter'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}